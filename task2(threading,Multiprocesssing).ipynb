{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23129a32-7b78-4fd8-8496-28ce2ab3d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary\n",
      "Sucessfull tasks :  [('img3', 'image processed successfully'), ('img2', 'image processed successfully')]\n",
      "Unsucessfull Tasks :  [('img4', 'Time increase 4 second'), ('img1', 'Time increase 4 second')]\n"
     ]
    }
   ],
   "source": [
    "#  Question 4 (Your Task)\n",
    "# You need to process a list of images using ProcessPoolExecutor.\n",
    "# Each image processing takes random time between 3 to 6 seconds.\n",
    "# Requirements:\n",
    "\n",
    "# If an image takes more than 4 seconds, cancel it or log it as \"Too Slow.\"\n",
    "\n",
    "# Process the rest normally and log the results.\n",
    "\n",
    "# ðŸ‘‰ Hints:\n",
    "# You will need to track start times.\n",
    "\n",
    "# You can explore wait() function from concurrent.futures.\n",
    "\n",
    "# Timeout handling is key.\n",
    "import random \n",
    "from concurrent.futures import ThreadPoolExecutor,as_completed\n",
    "import time\n",
    "start=time.perf_counter()\n",
    "end=time.perf_counter()\n",
    "def imageprocess(img):\n",
    "    duration = random.uniform(2, 6)\n",
    "    if duration > 4:\n",
    "        raise Exception(f\"Processing took too long: {duration:.2f} seconds\")\n",
    "    time.sleep(duration)\n",
    "    return \"image processed successfully\"\n",
    "    \n",
    "images=['img1','img2','img3','img4']\n",
    "success=[]\n",
    "timeout=[]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures={executor.submit(imageprocess,img):img for img in images}\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            success.append((futures[future],future.result()))\n",
    "        except Exception :\n",
    "            timeout.append((futures[future],\"Time increase 4 second\"))\n",
    "            #future.cancel()  only work on tasks which are not execued\n",
    "print(\"summary\")\n",
    "print(\"Sucessfull tasks : \", success)\n",
    "print(\"Unsucessfull Tasks : \",timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52499c1-91c1-4b56-a2fe-0f7aa9bf5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img3 processing for 3.11 seconds\n",
      "img1 processing for 3.43 seconds\n",
      "img4 processing for 4.19 seconds\n",
      "img2 processing for 5.30 seconds\n",
      "\n",
      "Summary:\n",
      "Successful tasks:  [('img3', 'img3 processed successfully in 3.11 seconds'), ('img1', 'img1 processed successfully in 3.43 seconds')]\n",
      "Unsuccessful tasks:  [('img4', 'img4 processing took too long: 4.19 seconds'), ('img2', 'img2 processing took too long: 5.30 seconds')]\n"
     ]
    }
   ],
   "source": [
    "!python processor.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f850f79b-7f44-4384-b736-b72a8d203e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from file1\n",
      "Downloading from file2\n",
      "Downloading from file3\n",
      "Downloading from file4\n",
      "Downloading from file5\n",
      "Success: Downloaded from file3\n",
      "Error: Failed to download file1\n",
      "Success: Downloaded from file2\n",
      "Success: Downloaded from file4\n",
      "Error: Failed to download file5\n",
      "\n",
      "Summary:\n",
      "Successes: ['Downloaded from file3', 'Downloaded from file2', 'Downloaded from file4']\n",
      "Failures: [('file1', 'Failed to download file1'), ('file5', 'Failed to download file5')]\n",
      "dict_values(['file1', 'file2', 'file3', 'file4', 'file5'])\n"
     ]
    }
   ],
   "source": [
    "# Problem:\n",
    "# You are downloading a list of files from multiple URLs using ThreadPoolExecutor.\n",
    "# Some files may fail to download (raise exceptions).\n",
    "# Write a program that:\n",
    "\n",
    "# Continues downloading all files even if some fail.\n",
    "\n",
    "# Collects both successful results and errors with proper logging.\n",
    "\n",
    "# Uses as_completed.\n",
    " from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "\n",
    "def download_file(url):\n",
    "    print(f\"Downloading from {url}\")\n",
    "    time.sleep(random.uniform(0.5, 2))  # Simulate variable download times\n",
    "    if random.random() < 0.3:  # 30% chance to fail\n",
    "        raise Exception(f\"Failed to download {url}\")\n",
    "    return f\"Downloaded from {url}\"\n",
    "\n",
    "urls = ['file1', 'file2', 'file3', 'file4', 'file5']\n",
    "\n",
    "successes = []\n",
    "failures = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(download_file, url): url for url in urls}\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        url = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            successes.append(result)\n",
    "            print(f\"Success: {result}\")\n",
    "        except Exception as e:\n",
    "            failures.append((url, str(e)))\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Successes:\", successes)\n",
    "print(\"Failures:\", failures)\n",
    "print(futures.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fece3d-4f9d-4b5d-8131-a5b9ff83eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 100: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 50: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 70: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 30: A process in the process pool was terminated abruptly while the future was running or pending.\n"
     ]
    }
   ],
   "source": [
    "# Question 2:\n",
    "# Problem:\n",
    "# You have a CPU-bound task (factorial calculation).\n",
    "# You need to:\n",
    "\n",
    "# Process a list of large numbers in parallel using ProcessPoolExecutor.\n",
    "\n",
    "# Retrieve results as they complete.\n",
    "\n",
    "# Ensure the fastest task is processed first, even if it was submitted last.\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "import math\n",
    "\n",
    "def compute_factorial(n):\n",
    "    time.sleep(2)  # Simulate heavy CPU work\n",
    "    return math.factorial(n)\n",
    "\n",
    "numbers = [100, 50, 70, 30]\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(compute_factorial, num): num for num in numbers}\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        number = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(f\"Factorial of {number} computed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {number}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c598de-8562-4f55-a0e2-6eb856ca4e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 10: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 20: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 30: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Error processing 40: A process in the process pool was terminated abruptly while the future was running or pending.\n"
     ]
    }
   ],
   "source": [
    "#make .py file of this to run it succesfully\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def heavy_cpu_task(n):\n",
    "    # Simulate CPU work: sum of squares up to n\n",
    "    result = sum(i * i for i in range(n))\n",
    "    return f\"Sum of squares up to {n} is {result}\"\n",
    "\n",
    "numbers = [10, 20, 30, 40]  # Large but manageable\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(heavy_cpu_task, num): num for num in numbers}\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        number = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(f\"Result for {number}: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {number}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed456f29-0121-4fee-a0a9-d1bce76806d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping site1\n",
      "Scraping site2\n",
      "Scraping site3\n",
      "Scraping site4\n",
      "Scraping site5\n",
      "Scraped data from site4\n",
      "Scraped data from site2\n",
      "Scraped data from site5\n",
      "Scraped data from site1\n",
      "Scraped data from site3\n",
      "\n",
      "Summary:\n",
      "Completed Sites: ['site4', 'site2', 'site5', 'site1', 'site3']\n",
      "Cancelled Sites: []\n"
     ]
    }
   ],
   "source": [
    "# Problem:\n",
    "# Use ThreadPoolExecutor to scrape a list of websites concurrently.\n",
    "# Requirements:\n",
    "\n",
    "# If total scraping takes more than 10 seconds, terminate remaining tasks.\n",
    "\n",
    "# Report which websites completed and which were cancelled.\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_site(site):\n",
    "    print(f\"Scraping {site}\")\n",
    "    time.sleep(random.uniform(2, 5))  # Simulate variable scraping times\n",
    "    return f\"Scraped data from {site}\"\n",
    "\n",
    "sites = ['site1', 'site2', 'site3', 'site4', 'site5']\n",
    "\n",
    "completed_sites = []\n",
    "cancelled_sites = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(scrape_site, site): site for site in sites}\n",
    "\n",
    "    try:\n",
    "        for future in as_completed(futures, timeout=10):\n",
    "            site = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                completed_sites.append(site)\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {site}: {e}\")\n",
    "    except TimeoutError:\n",
    "        print(\"\\nTimeout reached. Cancelling remaining tasks...\")\n",
    "\n",
    "    for future in futures:\n",
    "        if not future.done():\n",
    "            future.cancel()\n",
    "            cancelled_sites.append(futures[future])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Completed Sites:\", completed_sites)\n",
    "print(\"Cancelled Sites:\", cancelled_sites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff1814-e877-489b-abfb-dd6aa7ed4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â“ Question 5 (Your Task)\n",
    "# You are given two lists:\n",
    "# One list of I/O-bound tasks (web scraping URLs).\n",
    "# One list of CPU-bound tasks (data processing jobs).\n",
    "# Requirements:\n",
    "# Run I/O-bound tasks using ThreadPoolExecutor.\n",
    "# Run CPU-bound tasks using ProcessPoolExecutor.\n",
    "# Ensure both sets of tasks run concurrently.\n",
    "# Collect all results together.\n",
    "# ðŸ‘‰ Hints:\n",
    "# You will need to start both executors at the same time.\n",
    "# You can use as_completed() on both sets of futures together.\n",
    "# Think about using multiple with blocks or separate executor.submit callsâ“ Question 5 (Your Task)\n",
    "# You are given two lists:\n",
    "# One list of I/O-bound tasks (web scraping URLs).\n",
    "# One list of CPU-bound tasks (data processing jobs).\n",
    "\n",
    "# Requirements:\n",
    "\n",
    "# Run I/O-bound tasks using ThreadPoolExecutor.\n",
    "# Run CPU-bound tasks using ProcessPoolExecutor.\n",
    "# Ensure both sets of tasks run concurrently.\n",
    "# Collect all results together.\n",
    "\n",
    "# ðŸ‘‰ Hints:\n",
    "# You will need to start both executors at the same time.\n",
    "# You can use as_completed() on both sets of futures together.\n",
    "# Think about using multiple with blocks or separate executor.submit calls      \n",
    "#.py file\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "def fetch_data(url):\n",
    "    print(f\"Fetching from {url}\")\n",
    "    time.sleep(random.uniform(1, 3))  # Simulating network delay\n",
    "    return f\"Data from {url}\"\n",
    "\n",
    "def process_data(data):\n",
    "    print(f\"Processing {data}\")\n",
    "    time.sleep(random.uniform(2, 4))  # Simulating CPU-heavy task\n",
    "    result = sum(i * i for i in range(1000000))  # Simulated CPU load\n",
    "    return f\"Processed {data}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = ['site1.com', 'site2.com', 'site3.com', 'site4.com']\n",
    "    datasets = ['dataset1', 'dataset2', 'dataset3', 'dataset4']\n",
    "\n",
    "    all_futures = {}\n",
    "\n",
    "    with ThreadPoolExecutor() as thread_executor, ProcessPoolExecutor() as process_executor:\n",
    "        # Submit I/O tasks\n",
    "        for url in urls:\n",
    "            future = thread_executor.submit(fetch_data, url)\n",
    "            all_futures[future] = url\n",
    "\n",
    "        # Submit CPU tasks\n",
    "        for data in datasets:\n",
    "            future = process_executor.submit(process_data, data)\n",
    "            all_futures[future] = data\n",
    "\n",
    "        # Process results as they complete\n",
    "        for future in as_completed(all_futures):\n",
    "            task = all_futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(f\"Task {task} finished with result: {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Task {task} raised an exception: {e}\")\n",
    "\n",
    "    print(\"\\nAll tasks completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
